# Introduction to slurm and batch scripting
In this tutorial you will work through making a sbatch script, submitting this script to ASU's agave cluster, checking on the script's progress, and looking at the output from the script.

## Step 0 - Preparation
Before starting this tutorial, you will need to read through ASURC slurm tutorial: https://asurc.atlassian.net/wiki/spaces/RC/pages/45678605/Creating+SBATCH+scripts. We will run a part of the sbatch script provided in this tutorial.

## Step 1 - Log onto ASU's agave cluster
In order to run an sbatch script on ASU's agave cluster, you need to log onto the cluster. If you do not have an account, you can request one from here: https://cores.research.asu.edu/research-computing/get-started/create-an-account.

To log onto agave, open your terminal (or putty if you are on a windows device). Your login name is your asurite id and the password is the password you use to get onto myASU.

```
# Example of how to log onto Agave after opening terminal.
# Type the following
ssh your-asurite-id@agave.asu.edu
# Then press enter
```

## Step 2 - Create a sbatch script
Create the sbatch script from the ASURC slurm tutorial. Only copy lines 1-25 (see below). Save the sbatch script as `my_first_sbatch.sh`.

**Note:** You can make this file either on your local computer using a text editor of your choice and then transfer it to the cluster (using sftp to transfer the file) *or* you can make this file directly on the cluster using a text editor on the cluster like `vi`.

```
#!/bin/bash

#SBATCH -N 1  # number of nodes
#SBATCH -n 1  # number of "tasks" (default: allocates 1 core per task)
#SBATCH -t 0-00:02:00   # time in d-hh:mm:ss
#SBATCH -p serial       # partition
#SBATCH -q normal       # QOS
#SBATCH -o slurm.%j.out # file to save job's STDOUT (%j = JobId)
#SBATCH -e slurm.%j.err # file to save job's STDERR (%j = JobId)
#SBATCH --mail-type=ALL # Send an e-mail when a job starts, stops, or fails
#SBATCH --mail-user=%u@asu.edu # Mail-to address
#SBATCH --export=NONE   # Purge the job-submitting shell environment

# Always purge modules to ensure consistent environments
module purge    
# Load required modules for job's environment
module load anaconda/py3
##
# Generate 1,000,000 random numbers in bash,
#   then store sorted results
#   in `Distribution.txt`
##
for i in $(seq 1 1e6); do
  printf "%d\n" $RANDOM
done | sort -n > Distribution.txt
```

**Note:** Some of the SBATCH options can be customized/added/removed for the type of job you are running. See ASU RC guide linked above for more information on how to customize your sbatch script.

## Step 3 - Submit your sbatch script
To submit `my_first_sbatch.sh`, type the following in your terminal and then press enter.

```
sbatch my_first_sbatch.sh
```

Once you submit the job, you should see a jobID print to the screen. Save this ID (we will use this later to identify the output files generated by running the job).

## Step 4 - Check the progress of your sbatch script
You can check on the progress of your submitted sbatch script by using the following command.

```
# Look at the progress in the queue
squeue -u your-asurite-id
```

Because we work on shared resources on the cluster, your job will not always immediately start running. Also, depending on what commands you are running, some jobs may take hours or even days to complete. Because of this, it is important to be able to tell what is happening with your job - if it's just pending in the queue, running, or finished. To do this, use the above command. It will tell you if the job you have submitted is queued or running. It will also show if it has completed for a few seconds after finishing (after a few seconds it wonâ€™t be in the list anymore). Note that if you omit your username you can see everything queued on the cluster, but that is typically not useful at ASU because there are so many jobs constantly running or queued. Below is an example of what you will see with this command. `ST` is the status of the job and `PD` is bending and `R` will show if it is running and `CG` is canceled.

```
JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
8115681 yyanggpu1 RCAN_HAN ywan1053 CG       0:16      1 cg20-11
7949489      htc2 basaltco lheffern PD       0:00      8 (Priority)
7949488      htc2 basaltco lheffern PD       0:00      8 (Resources)
8083092 cmuhichcp W_Site_2 sawilso6 PD       0:00      1 (Resources)
8085123 cmuhichcp  far0.sh djriver7 PD       0:00      1 (Priority)
8115501       htc _interac amtarave  R    1:06:19      1 cg40-2
```

## Step 5 - Look at the output files generated from your sbatch script
Once your sbatch script has completed, you should see an `*.out` and `*.err` file in the directory where you submitted the `my_first_sbatch.sh`.

```
less slurm-<jobID>.out
less slurm-<jobID>.err
```

These files will contain the details of anything printed by the script (the `*.out` file) and any errors (the `*.err` file) encountered. A job that is not designed to print anything and that runs successfully may have a completely empty slurm file and that is okay. This is typically the first thing to look at after a job is complete to get a sense of if it completed successfully.


## Other helpful commands

```
sacct # shows the status of all your recently submitted jobs
scancel -u <your-asurite-id>  # cancels all jobs you have submitted
scancel <jobID> # cancels one specified job
```

## Resources for more information
https://support.ceci-hpc.be/doc/_contents/QuickStart/SubmittingJobs/SlurmTutorial.html
https://slurm.schedmd.com/tutorials.html
